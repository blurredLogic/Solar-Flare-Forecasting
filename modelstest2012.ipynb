{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xaVGEvniS7xo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load from local /content or from Drive\n",
        "X = np.load('X_2012.npy')   # shape (501318, 180, 2)\n",
        "y = np.load('y_2012.npy')   # shape (501318,)\n",
        "\n",
        "# Replace non-finite values with 0\n",
        "X[~np.isfinite(X)] = 0.0\n",
        "\n",
        "# Clip negatives to 0\n",
        "X = np.clip(X, 0.0, None)\n",
        "\n",
        "X = np.log10(X + 1e-10)\n",
        "\n",
        "mean = X.mean(axis=(0, 1), keepdims=True)\n",
        "std  = X.std(axis=(0, 1), keepdims=True) + 1e-6\n",
        "\n",
        "X = (X - mean) / std\n",
        "\n",
        "print(X.shape, y.shape)\n",
        "pd.Series(y).value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "ozfBDMQtTLVz",
        "outputId": "3ea1ec9e-bad2-4720-db78-bc6b471c150c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(501318, 180, 2) (501318,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "A    348001\n",
              "C    102287\n",
              "B     39801\n",
              "M     10619\n",
              "X       610\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>A</th>\n",
              "      <td>348001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C</th>\n",
              "      <td>102287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B</th>\n",
              "      <td>39801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>M</th>\n",
              "      <td>10619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X</th>\n",
              "      <td>610</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode string labels -> integers\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y_encoded, test_size=0.30, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "classes = np.unique(y_train)\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=classes,\n",
        "    y=y_train\n",
        ")\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "afgJXs0ZTNAh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Flatten\n",
        "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "X_val_flat   = X_val.reshape(X_val.shape[0], -1)\n",
        "X_test_flat  = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Optional: subsample for speed\n",
        "n_sub = 80000\n",
        "idx = np.random.choice(len(X_train_flat), size=n_sub, replace=False)\n",
        "X_train_sub = X_train_flat[idx]\n",
        "y_train_sub = y_train[idx]\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=None,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    class_weight='balanced_subsample'\n",
        ")\n",
        "\n",
        "rf.fit(X_train_sub, y_train_sub)\n",
        "\n",
        "y_pred_val = rf.predict(X_val_flat)\n",
        "print(\"Random Forest - Validation report:\")\n",
        "print(classification_report(y_val, y_pred_val, target_names=le.classes_))"
      ],
      "metadata": {
        "id": "eg-BxK_xT99i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c879ba5-0ff4-4b09-e4e4-02a2cc35d54a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest - Validation report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.76      0.99      0.86     52201\n",
            "           B       0.93      0.10      0.17      5970\n",
            "           C       0.94      0.35      0.51     15343\n",
            "           M       0.86      0.28      0.42      1593\n",
            "           X       1.00      0.41      0.58        91\n",
            "\n",
            "    accuracy                           0.78     75198\n",
            "   macro avg       0.90      0.43      0.51     75198\n",
            "weighted avg       0.81      0.78      0.73     75198\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GOESDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = y.astype(np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # X: (seq_len=180, features=2) -> (channels=2, seq_len=180)\n",
        "        x = self.X[idx].transpose(1, 0)\n",
        "        return x, self.y[idx]\n",
        "\n",
        "train_ds = GOESDataset(X_train, y_train)\n",
        "val_ds   = GOESDataset(X_val, y_val)\n",
        "test_ds  = GOESDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=512, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=512, shuffle=False)"
      ],
      "metadata": {
        "id": "WRGNuUlpUBh6"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN1D(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(2, 32, kernel_size=5, padding=2)\n",
        "        self.bn1   = nn.BatchNorm1d(32)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
        "        self.bn2   = nn.BatchNorm1d(64)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
        "        self.bn3   = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # global pooling over time dimension\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        self.fc = nn.Linear(128, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, 2, 180)\n",
        "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))  # -> (32, 90)\n",
        "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))  # -> (64, 45)\n",
        "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))  # -> (128, 22 or 23)\n",
        "        x = self.global_pool(x)   # -> (batch, 128, 1)\n",
        "        x = x.squeeze(-1)         # -> (batch, 128)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ke9SSUw8UDuh"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
        "        \"\"\"\n",
        "        alpha: tensor of shape [num_classes] with class weights\n",
        "        gamma: focusing parameter\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        # logits: (batch, num_classes)\n",
        "        # targets: (batch,)\n",
        "        ce_loss = F.cross_entropy(\n",
        "            logits, targets,\n",
        "            weight=self.alpha,\n",
        "            reduction='none'  # we handle reduction ourselves\n",
        "        )  # shape (batch,)\n",
        "\n",
        "        # pt is the probability of the true class\n",
        "        pt = torch.exp(-ce_loss)  # high for well-classified, low for misclassified\n",
        "\n",
        "        focal_term = (1 - pt) ** self.gamma\n",
        "        loss = focal_term * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        else:\n",
        "            return loss"
      ],
      "metadata": {
        "id": "WY40MRR3iuim"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Using focal loss:\", type(criterion))\n",
        "print(\"Class weights:\", class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsSbMVYbKg0W",
        "outputId": "06df0f72-4b7f-48e1-f3bd-a2ff7b63c302"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using focal loss: <class '__main__.FocalLoss'>\n",
            "Class weights: tensor([  0.2881,   2.5191,   0.9802,   9.4423, 164.3663])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "n_classes = len(le.classes_)\n",
        "model = CNN1D(n_classes).to(device)\n",
        "\n",
        "raw = class_weights.numpy()\n",
        "scaled = np.sqrt(raw)\n",
        "scaled = scaled / scaled.min()\n",
        "class_weights_tamed = torch.tensor(scaled, dtype=torch.float32)\n",
        "print(\"Tamed weights:\", class_weights_tamed)\n",
        "\n",
        "criterion = FocalLoss(alpha=class_weights_tamed.to(device), gamma=1.0)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for Xb, yb in loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(train):\n",
        "            outputs = model(Xb)\n",
        "            loss = criterion(outputs, yb)\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * yb.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "EPOCHS = 20\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train_loss, train_acc = run_epoch(train_loader, train=True)\n",
        "    val_loss, val_acc     = run_epoch(val_loader, train=False)\n",
        "    print(f\"Epoch {epoch:02d} | \"\n",
        "          f\"Train loss {train_loss:.4f}, acc {train_acc:.3f} | \"\n",
        "          f\"Val loss {val_loss:.4f}, acc {val_acc:.3f}\")"
      ],
      "metadata": {
        "id": "luBixyjbUHH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0359ecc2-724c-48b3-8d2c-a0e5f10255c1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Tamed weights: tensor([ 1.0000,  2.9569,  1.8445,  5.7248, 23.8850])\n",
            "Epoch 01 | Train loss 1.1538, acc 0.597 | Val loss 1.1700, acc 0.498\n",
            "Epoch 02 | Train loss 1.0821, acc 0.611 | Val loss 1.2890, acc 0.395\n",
            "Epoch 03 | Train loss 1.0439, acc 0.616 | Val loss 1.2970, acc 0.647\n",
            "Epoch 04 | Train loss 1.0116, acc 0.622 | Val loss 1.0390, acc 0.650\n",
            "Epoch 05 | Train loss 0.9797, acc 0.627 | Val loss 1.0973, acc 0.678\n",
            "Epoch 06 | Train loss 0.9485, acc 0.631 | Val loss 1.0052, acc 0.701\n",
            "Epoch 07 | Train loss 0.9161, acc 0.638 | Val loss 1.4685, acc 0.591\n",
            "Epoch 08 | Train loss 0.8817, acc 0.645 | Val loss 0.9939, acc 0.658\n",
            "Epoch 09 | Train loss 0.8493, acc 0.652 | Val loss 0.9685, acc 0.665\n",
            "Epoch 10 | Train loss 0.8253, acc 0.659 | Val loss 1.2489, acc 0.629\n",
            "Epoch 11 | Train loss 0.8014, acc 0.665 | Val loss 1.0287, acc 0.640\n",
            "Epoch 12 | Train loss 0.7800, acc 0.671 | Val loss 0.8101, acc 0.675\n",
            "Epoch 13 | Train loss 0.7576, acc 0.677 | Val loss 0.9496, acc 0.706\n",
            "Epoch 14 | Train loss 0.7389, acc 0.683 | Val loss 1.2486, acc 0.697\n",
            "Epoch 15 | Train loss 0.7216, acc 0.689 | Val loss 1.0027, acc 0.671\n",
            "Epoch 16 | Train loss 0.7060, acc 0.695 | Val loss 1.2427, acc 0.641\n",
            "Epoch 17 | Train loss 0.6890, acc 0.699 | Val loss 1.0725, acc 0.627\n",
            "Epoch 18 | Train loss 0.6767, acc 0.702 | Val loss 1.4750, acc 0.589\n",
            "Epoch 19 | Train loss 0.6627, acc 0.708 | Val loss 1.0220, acc 0.555\n",
            "Epoch 20 | Train loss 0.6498, acc 0.712 | Val loss 0.6923, acc 0.698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "all_true  = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in test_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        outputs = model(Xb)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_true.append(yb.cpu().numpy())\n",
        "\n",
        "all_preds = np.concatenate(all_preds)\n",
        "all_true  = np.concatenate(all_true)\n",
        "\n",
        "print(\"CNN Test classification report:\")\n",
        "print(classification_report(all_true, all_preds, target_names=le.classes_))\n",
        "\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion_matrix(all_true, all_preds))"
      ],
      "metadata": {
        "id": "GtRNymw6UK27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee8c706e-2245-4f74-e7f5-def8f693e58c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN Test classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           A       0.87      0.70      0.77     52200\n",
            "           B       0.46      0.67      0.55      5970\n",
            "           C       0.48      0.72      0.57     15343\n",
            "           M       0.67      0.59      0.62      1593\n",
            "           X       0.58      0.99      0.73        92\n",
            "\n",
            "    accuracy                           0.70     75198\n",
            "   macro avg       0.61      0.73      0.65     75198\n",
            "weighted avg       0.75      0.70      0.71     75198\n",
            "\n",
            "Confusion matrix:\n",
            "[[36428  4311 11164   242    55]\n",
            " [ 1506  4016   441     7     0]\n",
            " [ 3711   365 11046   213     8]\n",
            " [  164    10   484   933     2]\n",
            " [    1     0     0     0    91]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1Ô∏è‚É£ What these new results really mean\n",
        "\n",
        "RF (for comparison, still similar):\n",
        "\n",
        "Accuracy: 0.78\n",
        "\n",
        "C recall: 0.35\n",
        "\n",
        "M recall: 0.28\n",
        "\n",
        "X recall: 0.41\n",
        "\n",
        "So RF = high accuracy, but meh at catching real flares.\n",
        "\n",
        "CNN with focal loss + deeper net + log scaling:\n",
        "\n",
        "              precision    recall   support\n",
        "A              0.92        0.01     52200\n",
        "B              0.16        0.96      5970\n",
        "C              0.32        0.57     15343\n",
        "M              0.13        0.96      1593\n",
        "X              0.24        1.00        92\n",
        "\n",
        "accuracy: 0.22\n",
        "macro avg recall: 0.70\n",
        "\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "It almost completely stopped predicting A (only 1% of A are recognised as A).\n",
        "\n",
        "It is extremely good at recognising flares:\n",
        "\n",
        "B recall: 0.96\n",
        "\n",
        "C recall: 0.57\n",
        "\n",
        "M recall: 0.96\n",
        "\n",
        "X recall: 1.00\n",
        "\n",
        "So it‚Äôs basically saying:\n",
        "\n",
        "‚ÄúIf there is any chance of a flare, I scream FLARE, I don‚Äôt care about being wrong on quiet periods.‚Äù\n",
        "\n",
        "In operational space-weather terms:\n",
        "This is a hyper-sensitive ‚Äúnever miss a flare‚Äù mode.\n",
        "Tons of false alarms, very few missed flares.\n",
        "\n",
        "That‚Äôs actually a legit behaviour and something you can use in your thesis as one extreme of the trade-off.\n",
        "\n",
        "2Ô∏è‚É£ Why this is happening (intuition)\n",
        "\n",
        "Your class weights:\n",
        "\n",
        "A:  0.29\n",
        "B:  2.52\n",
        "C:  0.98\n",
        "M:  9.44\n",
        "X: 164.37\n",
        "\n",
        "\n",
        "And then we used focal loss with gamma=2:\n",
        "\n",
        "Rare classes (M, X) are hugely upweighted.\n",
        "\n",
        "Focal loss further boosts the contribution from misclassified hard examples.\n",
        "\n",
        "So the optimisation problem becomes:\n",
        "\n",
        "‚ÄúI would rather be wrong on thousands of A-class windows than miss even a few M/X flares.‚Äù\n",
        "\n",
        "The model listened. Hard. üòÖ\n",
        "\n",
        "So:\n",
        "\n",
        "Mathematically: the loss from misclassifying flares is massive compared to misclassifying A.\n",
        "\n",
        "So the network sacrifices almost all A accuracy to nail flares.\n",
        "\n",
        "In other words, we overcorrected the imbalance."
      ],
      "metadata": {
        "id": "kJbZ3LM7upx7"
      }
    }
  ]
}