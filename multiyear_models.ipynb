{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "54QP2170v5mr"
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0xfKcxCItmnd"
   },
   "outputs": [],
   "source": [
    "WINDOW_IN_MIN = 180   # 3 hours\n",
    "WINDOW_OUT_MIN = 60   # 1 hour ahead\n",
    "\n",
    "flare_order = {\"A\": 0, \"B\": 1, \"C\": 2, \"M\": 3, \"X\": 4}\n",
    "\n",
    "def get_highest_class(classes):\n",
    "    if len(classes) == 0:\n",
    "        return \"A\"\n",
    "    return max(classes, key=lambda c: flare_order[c])\n",
    "\n",
    "def make_future_labels(index, flare_df, horizon_min=60):\n",
    "    \"\"\"\n",
    "    index: DatetimeIndex of the 1-min irradiance series\n",
    "    flare_df: dataframe with ['flare_time', 'class_letter']\n",
    "    \"\"\"\n",
    "    fl_times = flare_df['flare_time'].values\n",
    "    fl_classes = flare_df['class_letter'].values\n",
    "    n_events = len(fl_times)\n",
    "\n",
    "    labels = []\n",
    "    j = 0\n",
    "\n",
    "    for t in index.values:\n",
    "        # advance pointer past any flares before t\n",
    "        while j < n_events and fl_times[j] < t:\n",
    "            j += 1\n",
    "\n",
    "        h_end = t + np.timedelta64(horizon_min, 'm')\n",
    "        k = j\n",
    "        classes = []\n",
    "\n",
    "        while k < n_events and fl_times[k] <= h_end:\n",
    "            classes.append(fl_classes[k])\n",
    "            k += 1\n",
    "\n",
    "        labels.append(get_highest_class(classes))\n",
    "\n",
    "    return np.array(labels)\n",
    "\n",
    "def build_windows(irrad_1m, labels, window_in_min=180):\n",
    "    data = irrad_1m[['short_xray', 'long_xray']].values\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    for i in range(window_in_min, len(irrad_1m)):\n",
    "        x_window = data[i-window_in_min:i, :]\n",
    "        X_list.append(x_window)\n",
    "        y_list.append(labels[i])\n",
    "\n",
    "    X = np.stack(X_list)\n",
    "    y = np.array(y_list)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5UpEbdQKtsAn"
   },
   "outputs": [],
   "source": [
    "def process_year(year, base_irrad_dir=\"\", base_flsum_dir=\"\"):\n",
    "    print(f\"\\n=== Processing year {year} ===\")\n",
    "    irrad_path = f\"{base_irrad_dir}irrad_{year}.nc\"\n",
    "    flsum_path = f\"{base_flsum_dir}flsum_{year}.nc\"\n",
    "\n",
    "    # 1) Load irrad\n",
    "    irrad_ds = xr.open_dataset(irrad_path)\n",
    "    irrad_df = irrad_ds[['a_flux', 'b_flux']].to_dataframe().reset_index()\n",
    "\n",
    "    irrad_df.rename(columns={\n",
    "        'time': 'timestamp',\n",
    "        'a_flux': 'short_xray',\n",
    "        'b_flux': 'long_xray'\n",
    "    }, inplace=True)\n",
    "\n",
    "    irrad_df['timestamp'] = pd.to_datetime(irrad_df['timestamp'])\n",
    "    irrad_df.set_index('timestamp', inplace=True)\n",
    "\n",
    "    # 2) Resample to 1-min\n",
    "    irrad_1m = irrad_df.resample('1min').mean().dropna()\n",
    "    print(\"1-min irradiance shape:\", irrad_1m.shape)\n",
    "\n",
    "    # 3) Load flare summary\n",
    "    flsum_ds = xr.open_dataset(flsum_path)\n",
    "    flsum_df = flsum_ds[['flare_class']].to_dataframe().reset_index()\n",
    "\n",
    "    flsum_df.rename(columns={'time': 'flare_time'}, inplace=True)\n",
    "    flsum_df['flare_time'] = pd.to_datetime(flsum_df['flare_time'])\n",
    "    flsum_df['class_letter'] = flsum_df['flare_class'].astype(str).str[0]\n",
    "\n",
    "    valid_classes = ['A', 'B', 'C', 'M', 'X']\n",
    "    flsum_df = flsum_df[flsum_df['class_letter'].isin(valid_classes)].copy()\n",
    "    flsum_df.sort_values('flare_time', inplace=True)\n",
    "    flsum_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(\"Number of flare events:\", len(flsum_df))\n",
    "\n",
    "    # 4) Future labels (next 1 hour)\n",
    "    labels = make_future_labels(irrad_1m.index, flsum_df, horizon_min=WINDOW_OUT_MIN)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "    # 5) Build sliding windows\n",
    "    X_year, y_year = build_windows(irrad_1m, labels, window_in_min=WINDOW_IN_MIN)\n",
    "    print(\"Year\", year, \"X shape:\", X_year.shape, \"y shape:\", y_year.shape)\n",
    "\n",
    "    return X_year, y_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pZWOLNcYt8ZN"
   },
   "outputs": [],
   "source": [
    "base_irrad_dir = \"C:/Users/ruthw/Desktop/irrad/\"\n",
    "base_flsum_dir = \"C:/Users/ruthw/Desktop/flsum/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXLeObvHum8B",
    "outputId": "b016437c-0c3f-443d-9f8e-a38d9ca7cbae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing year 2012 ===\n",
      "1-min irradiance shape: (501498, 2)\n",
      "Number of flare events: 3141\n",
      "Labels shape: (501498,)\n",
      "Year 2012 X shape: (501318, 180, 2) y shape: (501318,)\n",
      "\n",
      "=== Processing year 2013 ===\n",
      "1-min irradiance shape: (522024, 2)\n",
      "Number of flare events: 2975\n",
      "Labels shape: (522024,)\n",
      "Year 2013 X shape: (521844, 180, 2) y shape: (521844,)\n",
      "\n",
      "=== Processing year 2014 ===\n",
      "1-min irradiance shape: (512719, 2)\n",
      "Number of flare events: 3226\n",
      "Labels shape: (512719,)\n",
      "Year 2014 X shape: (512539, 180, 2) y shape: (512539,)\n",
      "Combined X: (1535701, 180, 2)\n",
      "Combined y: (1535701,)\n",
      "A    1072419\n",
      "C     326234\n",
      "B      95096\n",
      "M      38960\n",
      "X       2992\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "YEARS = [2012, 2013, 2014]  # adjust to what you have\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for year in YEARS:\n",
    "    X_y, y_y = process_year(year, base_irrad_dir, base_flsum_dir)\n",
    "    X_list.append(X_y)\n",
    "    y_list.append(y_y)\n",
    "\n",
    "X_all = np.concatenate(X_list, axis=0)\n",
    "y_all = np.concatenate(y_list, axis=0)\n",
    "\n",
    "print(\"Combined X:\", X_all.shape)\n",
    "print(\"Combined y:\", y_all.shape)\n",
    "print(pd.Series(y_all).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70dyB6ADurbc",
    "outputId": "5da48078-6159-4cfa-d817-b6f1a81d21f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After transform, any NaN? False  any Inf? False\n"
     ]
    }
   ],
   "source": [
    "# Clean\n",
    "X_all[~np.isfinite(X_all)] = 0.0\n",
    "X_all = np.clip(X_all, 0.0, None)\n",
    "\n",
    "# Log-transform\n",
    "X_all = np.log10(X_all + 1e-10)\n",
    "\n",
    "# Optional: global standardization\n",
    "mean = X_all.mean(axis=(0, 1), keepdims=True)\n",
    "std  = X_all.std(axis=(0, 1), keepdims=True) + 1e-6\n",
    "X_all = (X_all - mean) / std\n",
    "\n",
    "print(\"After transform, any NaN?\", np.isnan(X_all).any(), \" any Inf?\", np.isinf(X_all).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3NWEqHgUut3G"
   },
   "outputs": [],
   "source": [
    "np.save(\"X_multi.npy\", X_all)\n",
    "np.save(\"y_multi.npy\", y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QvG54JIXvTve",
    "outputId": "d377a80b-1e65-4c38-cccf-10afd665094f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([  0.2864,   3.2298,   0.9415,   7.8835, 102.6734])\n"
     ]
    }
   ],
   "source": [
    "X = np.load(\"X_multi.npy\")\n",
    "y = np.load(\"y_multi.npy\")\n",
    "\n",
    "# Manual encoding to keep class order fixed\n",
    "label_map = {'A': 0, 'B': 1, 'C': 2, 'M': 3, 'X': 4}\n",
    "y_encoded = np.array([label_map[c] for c in y])\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y_encoded, test_size=0.30, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "classes = np.array([0,1,2,3,4])\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_5aQwGCvcPa"
   },
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "8IE7gT__vg81",
    "outputId": "0287eb2f-d4e6-4b4b-8557-4af0f673a702"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1535701, 180, 2)\n",
      "y: (1535701,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "A    1072419\n",
       "C     326234\n",
       "B      95096\n",
       "M      38960\n",
       "X       2992\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"X:\", X.shape)\n",
    "print(\"y:\", y.shape)\n",
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9ec8JCKPvvvE"
   },
   "outputs": [],
   "source": [
    "label_map = {'A': 0, 'B': 1, 'C': 2, 'M': 3, 'X': 4}\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "y_encoded = np.array([label_map[c] for c in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1jQZ0M_UvxeR",
    "outputId": "06a0c4d4-82ab-4f08-dd5b-dd5f6415a5f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1074990, 180, 2) Val: (230355, 180, 2) Test: (230356, 180, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y_encoded, test_size=0.30, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ucp4rTwuvzdD",
    "outputId": "6cf6eef2-9618-49d3-b783-8c44d06152c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: [  0.28639937   3.22979855   0.94147063   7.88347023 102.67335244]\n",
      "Scaled: [ 1.          3.35816237  1.81308165  5.24653638 18.93402056]\n"
     ]
    }
   ],
   "source": [
    "classes = np.array([0,1,2,3,4])\n",
    "\n",
    "raw_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "# Tame extreme imbalance\n",
    "scaled_weights = np.sqrt(raw_weights)\n",
    "scaled_weights = scaled_weights / scaled_weights.min()\n",
    "\n",
    "class_weights = torch.tensor(scaled_weights, dtype=torch.float32)\n",
    "\n",
    "print(\"Raw:\", raw_weights)\n",
    "print(\"Scaled:\", scaled_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7iP2ZjZLv1hm"
   },
   "outputs": [],
   "source": [
    "class GOESDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # (180, 2) → (2, 180)\n",
    "        x = self.X[idx].transpose(1, 0)\n",
    "        return x, self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(GOESDataset(X_train, y_train), batch_size=256, shuffle=True)\n",
    "val_loader   = DataLoader(GOESDataset(X_val, y_val), batch_size=512, shuffle=False)\n",
    "test_loader  = DataLoader(GOESDataset(X_test, y_test), batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4eMrlmqHwFV6"
   },
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(2, 32, kernel_size=5, padding=2)\n",
    "        self.bn1   = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
    "        self.bn2   = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "        self.bn3   = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.global_pool(x).squeeze(-1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tM65DXiMwMob"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce = F.cross_entropy(logits, targets, weight=self.alpha, reduction='none')\n",
    "        pt = torch.exp(-ce)\n",
    "        loss = ((1 - pt) ** self.gamma) * ce\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "yqX_zxKKwOvK"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CNN1D(n_classes=5).to(device)\n",
    "criterion = FocalLoss(alpha=class_weights.to(device), gamma=1.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for Xb, yb in loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            out = model(Xb)\n",
    "            loss = criterion(out, yb)\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(yb)\n",
    "        preds = out.argmax(1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += len(yb)\n",
    "\n",
    "    return total_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "lOBEIYv4yFgi",
    "outputId": "4e61ca9c-50aa-490d-e32c-9397ae64b6b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train loss 1.1748, acc 0.609 | Val loss 1.2101, acc 0.512\n",
      "Epoch 02 | Train loss 1.1265, acc 0.625 | Val loss 1.1417, acc 0.648\n",
      "Epoch 03 | Train loss 1.1003, acc 0.631 | Val loss 1.0935, acc 0.703\n",
      "Epoch 04 | Train loss 1.0744, acc 0.633 | Val loss 1.2434, acc 0.627\n",
      "Epoch 05 | Train loss 1.0480, acc 0.633 | Val loss 1.0287, acc 0.658\n",
      "Epoch 06 | Train loss 1.0214, acc 0.634 | Val loss 1.0754, acc 0.690\n",
      "Epoch 07 | Train loss 0.9962, acc 0.636 | Val loss 1.8268, acc 0.382\n",
      "Epoch 08 | Train loss 0.9717, acc 0.638 | Val loss 1.2128, acc 0.526\n",
      "Epoch 09 | Train loss 0.9466, acc 0.642 | Val loss 1.0219, acc 0.573\n",
      "Epoch 10 | Train loss 0.9247, acc 0.644 | Val loss 1.5312, acc 0.470\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    train_loss, train_acc = run_epoch(train_loader, train=True)\n",
    "    val_loss, val_acc     = run_epoch(val_loader, train=False)\n",
    "\n",
    "    print(f\"Epoch {ep:02d} | \"\n",
    "          f\"Train loss {train_loss:.4f}, acc {train_acc:.3f} | \"\n",
    "          f\"Val loss {val_loss:.4f}, acc {val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GNm4ah_4wRPr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.78      0.53      0.63    160863\n",
      "           B       0.10      0.59      0.17     14265\n",
      "           C       0.44      0.24      0.31     48935\n",
      "           M       0.27      0.36      0.31      5844\n",
      "           X       0.32      0.21      0.25       449\n",
      "\n",
      "    accuracy                           0.47    230356\n",
      "   macro avg       0.38      0.39      0.33    230356\n",
      "weighted avg       0.65      0.47      0.53    230356\n",
      "\n",
      "[[85686 58685 13527  2861   104]\n",
      " [ 5847  8396    15     7     0]\n",
      " [16836 17529 11798  2736    36]\n",
      " [ 1524   787  1397  2079    57]\n",
      " [  113    23   120    99    94]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_true = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in test_loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        out = model(Xb)\n",
    "        preds = out.argmax(1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_true.append(yb.cpu().numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_true  = np.concatenate(all_true)\n",
    "\n",
    "print(classification_report(all_true, all_preds,\n",
    "                            target_names=['A','B','C','M','X']))\n",
    "\n",
    "print(confusion_matrix(all_true, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary ≥C classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    <C (A/B)       0.81      0.91      0.86    175128\n",
      "  ≥C (C/M/X)       0.53      0.33      0.41     55228\n",
      "\n",
      "    accuracy                           0.77    230356\n",
      "   macro avg       0.67      0.62      0.63    230356\n",
      "weighted avg       0.74      0.77      0.75    230356\n",
      "\n",
      "Confusion matrix (≥C):\n",
      "[[158614  16514]\n",
      " [ 36812  18416]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Ground truth: ≥C\n",
    "y_true_C = (all_true >= 2).astype(int)\n",
    "y_pred_C = (all_preds >= 2).astype(int)\n",
    "\n",
    "print(\"Binary ≥C classification report:\")\n",
    "print(classification_report(\n",
    "    y_true_C,\n",
    "    y_pred_C,\n",
    "    target_names=[\"<C (A/B)\", \"≥C (C/M/X)\"]\n",
    "))\n",
    "\n",
    "print(\"Confusion matrix (≥C):\")\n",
    "print(confusion_matrix(y_true_C, y_pred_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary ≥M classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  <M (A/B/C)       0.98      0.97      0.98    224063\n",
      "    ≥M (M/X)       0.29      0.37      0.32      6293\n",
      "\n",
      "    accuracy                           0.96    230356\n",
      "   macro avg       0.64      0.67      0.65    230356\n",
      "weighted avg       0.96      0.96      0.96    230356\n",
      "\n",
      "Confusion matrix (≥M):\n",
      "[[218319   5744]\n",
      " [  3964   2329]]\n"
     ]
    }
   ],
   "source": [
    "# Ground truth: ≥M\n",
    "y_true_M = (all_true >= 3).astype(int)\n",
    "y_pred_M = (all_preds >= 3).astype(int)\n",
    "\n",
    "print(\"Binary ≥M classification report:\")\n",
    "print(classification_report(\n",
    "    y_true_M,\n",
    "    y_pred_M,\n",
    "    target_names=[\"<M (A/B/C)\", \"≥M (M/X)\"]\n",
    "))\n",
    "\n",
    "print(\"Confusion matrix (≥M):\")\n",
    "print(confusion_matrix(y_true_M, y_pred_M))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
