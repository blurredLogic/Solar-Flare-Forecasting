{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "54QP2170v5mr"
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0xfKcxCItmnd"
   },
   "outputs": [],
   "source": [
    "WINDOW_IN_MIN = 180   # 3 hours window for input\n",
    "WINDOW_OUT_MIN = 60   # 1 hour ahead for output\n",
    "\n",
    "flare_order = {\"A\": 0, \"B\": 1, \"C\": 2, \"M\": 3, \"X\": 4}\n",
    "\n",
    "def get_highest_class(classes):\n",
    "    if len(classes) == 0:\n",
    "        return \"A\"\n",
    "    return max(classes, key=lambda c: flare_order[c])\n",
    "\n",
    "def make_future_labels(index, flare_df, horizon_min=60):\n",
    "\n",
    "    # DatetimeIndex of the 1-min irradiance series\n",
    "    # flare_df: dataframe with ['flare_time', 'class_letter']\n",
    "\n",
    "    fl_times = flare_df['flare_time'].values\n",
    "    fl_classes = flare_df['class_letter'].values\n",
    "    n_events = len(fl_times)\n",
    "\n",
    "    labels = []\n",
    "    j = 0\n",
    "\n",
    "    for t in index.values:\n",
    "        # advance pointer past any flares before t\n",
    "        while j < n_events and fl_times[j] < t:\n",
    "            j += 1\n",
    "\n",
    "        h_end = t + np.timedelta64(horizon_min, 'm')\n",
    "        k = j\n",
    "        classes = []\n",
    "\n",
    "        while k < n_events and fl_times[k] <= h_end:\n",
    "            classes.append(fl_classes[k])\n",
    "            k += 1\n",
    "\n",
    "        labels.append(get_highest_class(classes))\n",
    "\n",
    "    return np.array(labels)\n",
    "\n",
    "def build_windows(irrad_1m, labels, window_in_min=180):\n",
    "    data = irrad_1m[['short_xray', 'long_xray']].values\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    for i in range(window_in_min, len(irrad_1m)):\n",
    "        x_window = data[i-window_in_min:i, :]\n",
    "        X_list.append(x_window)\n",
    "        y_list.append(labels[i])\n",
    "\n",
    "    X = np.stack(X_list)\n",
    "    y = np.array(y_list)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5UpEbdQKtsAn"
   },
   "outputs": [],
   "source": [
    "def process_year(year, base_irrad_dir=\"\", base_flsum_dir=\"\"):\n",
    "    print(f\"\\n=== Processing year {year} ===\")\n",
    "    irrad_path = f\"{base_irrad_dir}irrad_{year}.nc\"\n",
    "    flsum_path = f\"{base_flsum_dir}flsum_{year}.nc\"\n",
    "\n",
    "    # 1) Load irrad\n",
    "    irrad_ds = xr.open_dataset(irrad_path)\n",
    "    irrad_df = irrad_ds[['a_flux', 'b_flux']].to_dataframe().reset_index()\n",
    "\n",
    "    irrad_df.rename(columns={\n",
    "        'time': 'timestamp',\n",
    "        'a_flux': 'short_xray',\n",
    "        'b_flux': 'long_xray'\n",
    "    }, inplace=True)\n",
    "\n",
    "    irrad_df['timestamp'] = pd.to_datetime(irrad_df['timestamp'])\n",
    "    irrad_df.set_index('timestamp', inplace=True)\n",
    "\n",
    "    # 2) Resample to 1-min\n",
    "    irrad_1m = irrad_df.resample('1min').mean().dropna()\n",
    "    print(\"1-min irradiance shape:\", irrad_1m.shape)\n",
    "\n",
    "    # 3) Load flare summary\n",
    "    flsum_ds = xr.open_dataset(flsum_path)\n",
    "    flsum_df = flsum_ds[['flare_class']].to_dataframe().reset_index()\n",
    "\n",
    "    flsum_df.rename(columns={'time': 'flare_time'}, inplace=True)\n",
    "    flsum_df['flare_time'] = pd.to_datetime(flsum_df['flare_time'])\n",
    "    flsum_df['class_letter'] = flsum_df['flare_class'].astype(str).str[0]\n",
    "\n",
    "    valid_classes = ['A', 'B', 'C', 'M', 'X']\n",
    "    flsum_df = flsum_df[flsum_df['class_letter'].isin(valid_classes)].copy()\n",
    "    flsum_df.sort_values('flare_time', inplace=True)\n",
    "    flsum_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(\"Number of flare events:\", len(flsum_df))\n",
    "\n",
    "    # 4) Future labels (next 1 hour)\n",
    "    labels = make_future_labels(irrad_1m.index, flsum_df, horizon_min=WINDOW_OUT_MIN)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "    # 5) Build sliding windows\n",
    "    X_year, y_year = build_windows(irrad_1m, labels, window_in_min=WINDOW_IN_MIN)\n",
    "    print(\"Year\", year, \"X shape:\", X_year.shape, \"y shape:\", y_year.shape)\n",
    "\n",
    "    return X_year, y_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pZWOLNcYt8ZN"
   },
   "outputs": [],
   "source": [
    "base_irrad_dir = \"C:/Users/ruthw/Desktop/irrad/\"\n",
    "base_flsum_dir = \"C:/Users/ruthw/Desktop/flsum/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXLeObvHum8B",
    "outputId": "b016437c-0c3f-443d-9f8e-a38d9ca7cbae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing year 2016 ===\n",
      "1-min irradiance shape: (501754, 2)\n",
      "Number of flare events: 2084\n",
      "Labels shape: (501754,)\n",
      "Year 2016 X shape: (501574, 180, 2) y shape: (501574,)\n",
      "\n",
      "=== Processing year 2017 ===\n",
      "1-min irradiance shape: (497854, 2)\n",
      "Number of flare events: 1541\n",
      "Labels shape: (497854,)\n",
      "Year 2017 X shape: (497674, 180, 2) y shape: (497674,)\n",
      "\n",
      "=== Processing year 2018 ===\n",
      "1-min irradiance shape: (499698, 2)\n",
      "Number of flare events: 409\n",
      "Labels shape: (499698,)\n",
      "Year 2018 X shape: (499518, 180, 2) y shape: (499518,)\n",
      "Combined X: (1498766, 180, 2)\n",
      "Combined y: (1498766,)\n",
      "A    1302308\n",
      "B     148551\n",
      "C      43148\n",
      "M       4393\n",
      "X        366\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "YEARS = [2016, 2017, 2018]  # adjust to what you have\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for year in YEARS:\n",
    "    X_y, y_y = process_year(year, base_irrad_dir, base_flsum_dir)\n",
    "    X_list.append(X_y)\n",
    "    y_list.append(y_y)\n",
    "\n",
    "X_all = np.concatenate(X_list, axis=0)\n",
    "y_all = np.concatenate(y_list, axis=0)\n",
    "\n",
    "print(\"Combined X:\", X_all.shape)\n",
    "print(\"Combined y:\", y_all.shape)\n",
    "print(pd.Series(y_all).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70dyB6ADurbc",
    "outputId": "5da48078-6159-4cfa-d817-b6f1a81d21f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After transform, any NaN? False  any Inf? False\n"
     ]
    }
   ],
   "source": [
    "# Clean\n",
    "X_all[~np.isfinite(X_all)] = 0.0\n",
    "X_all = np.clip(X_all, 0.0, None)\n",
    "\n",
    "# Log-transform\n",
    "X_all = np.log10(X_all + 1e-10)\n",
    "\n",
    "# Optional: global standardization\n",
    "mean = X_all.mean(axis=(0, 1), keepdims=True)\n",
    "std  = X_all.std(axis=(0, 1), keepdims=True) + 1e-6\n",
    "X_all = (X_all - mean) / std\n",
    "\n",
    "print(\"After transform, any NaN?\", np.isnan(X_all).any(), \" any Inf?\", np.isinf(X_all).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "3NWEqHgUut3G"
   },
   "outputs": [],
   "source": [
    "np.save(\"X_multi.npy\", X_all)\n",
    "np.save(\"y_multi.npy\", y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QvG54JIXvTve",
    "outputId": "d377a80b-1e65-4c38-cccf-10afd665094f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([2.3017e-01, 2.0178e+00, 6.9470e+00, 6.8236e+01, 8.1964e+02])\n"
     ]
    }
   ],
   "source": [
    "X = np.load(\"X_multi.npy\")\n",
    "y = np.load(\"y_multi.npy\")\n",
    "\n",
    "# Manual encoding to keep class order fixed\n",
    "label_map = {'A': 0, 'B': 1, 'C': 2, 'M': 3, 'X': 4}\n",
    "y_encoded = np.array([label_map[c] for c in y])\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y_encoded, test_size=0.30, random_state=42, stratify=y_encoded\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "classes = np.array([0,1,2,3,4])\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_5aQwGCvcPa"
   },
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "8IE7gT__vg81",
    "outputId": "0287eb2f-d4e6-4b4b-8557-4af0f673a702"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1498766, 180, 2)\n",
      "y: (1498766,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "A    1302308\n",
       "B     148551\n",
       "C      43148\n",
       "M       4393\n",
       "X        366\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"X:\", X.shape)\n",
    "print(\"y:\", y.shape)\n",
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "9ec8JCKPvvvE"
   },
   "outputs": [],
   "source": [
    "label_map = {'A': 0, 'B': 1, 'C': 2, 'M': 3, 'X': 4}\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "y_encoded = np.array([label_map[c] for c in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1jQZ0M_UvxeR",
    "outputId": "06a0c4d4-82ab-4f08-dd5b-dd5f6415a5f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1049136, 180, 2) Val: (224815, 180, 2) Test: (224815, 180, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y_encoded, test_size=0.30, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ucp4rTwuvzdD",
    "outputId": "6cf6eef2-9618-49d3-b783-8c44d06152c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: [2.30170851e-01 2.01784086e+00 6.94700040e+00 6.82364878e+01\n",
      " 8.19637500e+02]\n",
      "Scaled: [ 1.          2.96086288  5.49380829 17.21801854 59.67408226]\n"
     ]
    }
   ],
   "source": [
    "classes = np.array([0,1,2,3,4])\n",
    "\n",
    "raw_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "# Tame extreme imbalance\n",
    "scaled_weights = np.sqrt(raw_weights)\n",
    "scaled_weights = scaled_weights / scaled_weights.min()\n",
    "\n",
    "class_weights = torch.tensor(scaled_weights, dtype=torch.float32)\n",
    "\n",
    "print(\"Raw:\", raw_weights)\n",
    "print(\"Scaled:\", scaled_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "7iP2ZjZLv1hm"
   },
   "outputs": [],
   "source": [
    "class GOESDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # (180, 2) → (2, 180)\n",
    "        x = self.X[idx].transpose(1, 0)\n",
    "        return x, self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(GOESDataset(X_train, y_train), batch_size=256, shuffle=True)\n",
    "val_loader   = DataLoader(GOESDataset(X_val, y_val), batch_size=512, shuffle=False)\n",
    "test_loader  = DataLoader(GOESDataset(X_test, y_test), batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "4eMrlmqHwFV6"
   },
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(2, 32, kernel_size=5, padding=2)\n",
    "        self.bn1   = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
    "        self.bn2   = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "        self.bn3   = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.global_pool(x).squeeze(-1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "tM65DXiMwMob"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce = F.cross_entropy(logits, targets, weight=self.alpha, reduction='none')\n",
    "        pt = torch.exp(-ce)\n",
    "        loss = ((1 - pt) ** self.gamma) * ce\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "yqX_zxKKwOvK"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CNN1D(n_classes=5).to(device)\n",
    "criterion = FocalLoss(alpha=class_weights.to(device), gamma=1.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for Xb, yb in loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            out = model(Xb)\n",
    "            loss = criterion(out, yb)\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * len(yb)\n",
    "        preds = out.argmax(1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += len(yb)\n",
    "\n",
    "    return total_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "lOBEIYv4yFgi",
    "outputId": "4e61ca9c-50aa-490d-e32c-9397ae64b6b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train loss 0.7769, acc 0.776 | Val loss 0.7422, acc 0.792\n",
      "Epoch 02 | Train loss 0.7384, acc 0.793 | Val loss 0.7695, acc 0.683\n",
      "Epoch 03 | Train loss 0.7164, acc 0.797 | Val loss 0.7041, acc 0.766\n",
      "Epoch 04 | Train loss 0.6959, acc 0.798 | Val loss 0.8490, acc 0.674\n",
      "Epoch 05 | Train loss 0.6756, acc 0.801 | Val loss 0.6644, acc 0.802\n",
      "Epoch 06 | Train loss 0.6544, acc 0.804 | Val loss 0.7340, acc 0.642\n",
      "Epoch 07 | Train loss 0.6320, acc 0.806 | Val loss 0.6219, acc 0.797\n",
      "Epoch 08 | Train loss 0.6128, acc 0.809 | Val loss 0.6053, acc 0.800\n",
      "Epoch 09 | Train loss 0.5973, acc 0.811 | Val loss 0.6176, acc 0.759\n",
      "Epoch 10 | Train loss 0.5773, acc 0.815 | Val loss 0.6441, acc 0.749\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    train_loss, train_acc = run_epoch(train_loader, train=True)\n",
    "    val_loss, val_acc     = run_epoch(val_loader, train=False)\n",
    "\n",
    "    print(f\"Epoch {ep:02d} | \"\n",
    "          f\"Train loss {train_loss:.4f}, acc {train_acc:.3f} | \"\n",
    "          f\"Val loss {val_loss:.4f}, acc {val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "GNm4ah_4wRPr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.95      0.77      0.85    195347\n",
      "           B       0.27      0.67      0.38     22282\n",
      "           C       0.36      0.60      0.45      6472\n",
      "           M       0.29      0.60      0.39       659\n",
      "           X       0.47      0.85      0.61        55\n",
      "\n",
      "    accuracy                           0.75    224815\n",
      "   macro avg       0.47      0.70      0.54    224815\n",
      "weighted avg       0.87      0.75      0.79    224815\n",
      "\n",
      "[[149524  39458   5696    633     36]\n",
      " [  6382  14884   1008      8      0]\n",
      " [   633   1586   3890    347     16]\n",
      " [    33     66    161    398      1]\n",
      " [     0      0      2      6     47]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_true = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in test_loader:\n",
    "        Xb, yb = Xb.to(device), yb.to(device)\n",
    "        out = model(Xb)\n",
    "        preds = out.argmax(1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_true.append(yb.cpu().numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_true  = np.concatenate(all_true)\n",
    "\n",
    "print(classification_report(all_true, all_preds,\n",
    "                            target_names=['A','B','C','M','X']))\n",
    "\n",
    "print(confusion_matrix(all_true, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary ≥C classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    <C (A/B)       0.99      0.97      0.98    217629\n",
      "  ≥C (C/M/X)       0.40      0.68      0.50      7186\n",
      "\n",
      "    accuracy                           0.96    224815\n",
      "   macro avg       0.69      0.82      0.74    224815\n",
      "weighted avg       0.97      0.96      0.96    224815\n",
      "\n",
      "Confusion matrix (≥C):\n",
      "[[210248   7381]\n",
      " [  2318   4868]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Ground truth: ≥C\n",
    "y_true_C = (all_true >= 2).astype(int)\n",
    "y_pred_C = (all_preds >= 2).astype(int)\n",
    "\n",
    "print(\"Binary ≥C classification report:\")\n",
    "print(classification_report(\n",
    "    y_true_C,\n",
    "    y_pred_C,\n",
    "    target_names=[\"<C (A/B)\", \"≥C (C/M/X)\"]\n",
    "))\n",
    "\n",
    "print(\"Confusion matrix (≥C):\")\n",
    "print(confusion_matrix(y_true_C, y_pred_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary ≥M classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  <M (A/B/C)       1.00      1.00      1.00    224101\n",
      "    ≥M (M/X)       0.30      0.63      0.41       714\n",
      "\n",
      "    accuracy                           0.99    224815\n",
      "   macro avg       0.65      0.81      0.70    224815\n",
      "weighted avg       1.00      0.99      1.00    224815\n",
      "\n",
      "Confusion matrix (≥M):\n",
      "[[223061   1040]\n",
      " [   262    452]]\n"
     ]
    }
   ],
   "source": [
    "# Ground truth: ≥M\n",
    "y_true_M = (all_true >= 3).astype(int)\n",
    "y_pred_M = (all_preds >= 3).astype(int)\n",
    "\n",
    "print(\"Binary ≥M classification report:\")\n",
    "print(classification_report(\n",
    "    y_true_M,\n",
    "    y_pred_M,\n",
    "    target_names=[\"<M (A/B/C)\", \"≥M (M/X)\"]\n",
    "))\n",
    "\n",
    "print(\"Confusion matrix (≥M):\")\n",
    "print(confusion_matrix(y_true_M, y_pred_M))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
